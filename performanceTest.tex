\chapter{Performance and Source Code Analysis}
A major goal for any interactive Web application is to minimize the response time for user actions, because it has a significant impact on the user-experience. Users most often don't have patience to sit around and wait for slow page requests, which in some cases could result in Users abandoning the site in favor of other competitors.  The response time for such an action is a measurement of the time it takes from when the User initiates the action, until the result is completely visible in the browser. An action might be that the User clicks a button, follows a hyperlink, presses the enter key in a search field etc.

The response time depends on many performance factors in the Web app. In general, these are:
\begin{description}
\item[Application server's throughput], which concerns how many requests the application server can handle per time unit. 
\item [Database server's throughput], which concerns how many transactions the database can handle per time unit
\item[Client-tier efficiency] which concerns how fast the browser can render and display the result of a User-action. Often this depends on the JavaScript implementation that is required to display the result.
\end{description}
There are also other factors that affects the response time of a web-app, like network performance and the hardware that hosts the client and the server. However, these issues will not be considered in this thesis, mostly because performance tuning these elements does not directly indicate any pros or cons in the two Web architectures that are studied in this thesis. 

The application server's performance also depends on the scalability of the system. Scalability is a measurement of resilience under ever-increasing load. Hence another goal is to maintain the performance levels when the number of concurrent users increases. Scalability also depends on the application server's throughput, and the database server's throughput. 

Now, in order to analyze and compare performance and scalability properties for the two architectures, a number of system tests have been designed and executed in real deployment scenarios. Equal tests have been run on both prototypes. Now, one important thing to mention is that the application server's throughput also depends on the web framework that is used. So considering the fact that the architectures uses two different frameworks (Node Js and Java Swing), might cause subjective results for comparisons. However, I was aware of this fact before I chose to use two different web frameworks, and therefore I  built Architecture 2.0 with Spring as well (not really a big effort, just a matter of implementing the API in Java). The performance results for Architecture 2.0 on Spring vs Architecture 2.0 were very similar, and therefore I chose to do all the testing based on the Node Js version.

A final goal for this project is to analyze and compare the source code for the two architectures. The source code is to be measured in terms of code flexibility and maintainability, which is an important property in order to facilitate future code modifications and extensions. Now, as mentioned in the introduction chapter, such a comparison is a difficult and laborious task. Therefore the analysis of the source codes is somewhat short. However, a proper test case has been designed and implemented on both architectures, and the results are relevant. We will look at this in the end of the chapter.

\section{Hardware and software used for testing}
Both prototypes have been deployed on a test machine stationed in Madrid, Spain. The machine was hosted by PlanetLab, a global network of computers made available for researchers to develop, deploy and test distributed systems. The reason I chose to deploy it in Spain was in order to get realistic transmission times. The system specifications for the test machine are as follows: 

\newcolumntype{R}{>{\raggedleft\arraybackslash}X}%
\begin{tabularx}{\textwidth}{ | l | R | }
  \hline
  Operation System & Fedora 12  \\ \hline
  CPU Architecture & Intel(R) Core(TM)2 Quad CPU Q8400 \\ \hline
  CPU Clock Speed & 2.66GHz \\ \hline
  CPU Cache Size & 2048 KB \\ \hline  
\end{tabularx}

Performance testing was made with the Chrome Developer Tools, \cite{chrome} which is a browser feature for the Chrome browser that captures Url requests, and monitors JavaScript executions. The tool not only calculates the complete response time, but also gives a detailed overview of the times spent for each individual Web resource that is fetched from the server. Both Mozilla Firefox and Internet Explorer have similar tools for web performance testing and profiling, and they mostly deliver the same functionality. I chose to use Chrome simply because I am familiar with it, and it doesn't require any extra plugin installation. 
Stress testing the server was done with Apache JMeter, which is a Java program that is able to execute and monitor multiple threads. These threads can be configured to do Http requests.

\section{Performance and Scalability Tests}
In this section, we look at the concrete tests that have been made, and the results of these. The tests are separated in five different sections. The first two tests investigate the performance of the prototypes by inspecting request response times, the third test investigates the scalability characteristics of the prototypes by stress testing the back-end implementations, the fourth investigates database speed, and the last test investigates the architectures' source code in terms of flexibility and maintainability. What is missing here, is a test case that investigates the database's throughput properties. Unfortunately I did not have time to create this test. 

For testing purposes, an equal set of dummy objects were created in the two databases. These are:
\begin{itemize}
\item{} 1000 Shredders 
\item{} For every Shredder, a number from 0 til 10 fanees
\item{} 100000 Shreds
\item{} 100000 Battles
\item{} 10000 Battle requests
\end{itemize}

%% FORTSETT HER!!
\subsection{Test 1 - Page Loading Tests}
\textit{Goal: To determine how fast the prototypes create the major pages of Shredhub. Also, to investigate the times spent in the various phases during a request}\\
\textit{Tests: Response time/round-trip time}\\
The page loading test is meant to investigate the amount of time the User has to wait from the time a Url is requested, until the  page is displayed in the browser. This test is important in order to identify which of the two prototypes are capable of creating a page the fastest.  In addition to calculating the response times, the test also performs profiling of the various phases of the page loading processes, in order to identify how much time is spent on the server, and how much time is spent preparing the page in the browser.

The page loading test was performed actively by a human user. The tester would either, for the initial page, write the url in the address bar of a Web browser and press the enter button, otherwise, the tester would click on a link inside Shredhub that leads to the given page. I have chosen to use the Chrome web browser to run the tests, because it comes with the Chrome developer tools. Every test was done 5 times, in which the test results show the average of these.

The result figures show the complete round trip times for all the different server requests. These results are displayed in waterfall figures, which naturally depict the ordering of the different resource fetches. The waterfall models also capture the fact that some resources depend on each other in order to start fetching. For example, the browser will start fetch CSS/JS resources as soon as it has gotten the Html page from the server. For every test result, there is a number saying how much time was spent on the server, how much data was sent from the server (without images), and when it was fair to say that the page was visible in the browser. The latter timing represents the time when everything but the images were completely rendered. This is because rendering images is the last thing the browser does (in the case for Shredhub), and it says nothing about the performance differences for the two prototypes. The timings in the result figures are displayed in milliseconds. For Architecture 2.0, I have chosen to indicate the amount of time spent doing Ajax requests to the server (the timings concerns the time it took on the server + Http transmission times), as well as showing the time of the last JavaScript execution in the App. The last JavaScript execution indicates the time when the App has rendered every necessary Html template and written this to the Dom. Hence this is when I acknowledge that the page is finished and displayed in the browser. As for Architecture 1.0, the first request in the waterfall model always represents the time it took to process the request on the server plus the Http transmission times regarding this. I acknowledge that the page is finished and displayed in the browser when the DomContentLoad event is triggered, because this is when the browser has rendered every element in the Html page. 

 Note that the tests were performed on a somewhat slow network connection. This explains the slow timings, especially for image resources. However, because the testing on the two prototypes was done on the same network connection, the network overhead isn't relevant for the test results. 

The Urls that have been tested are:
\begin{enumerate}
\item \textit{www.shredhub.com/}
\item \textit{www.shredhub.com/theshredpool}
\item \textit{www.shredhub.com/shredders}
\item \textit{www.shredhub.com/shredder/1234}
\end{enumerate}
These are the four main pages of Shredhub. They all require database lookups in order for the pages to be generated. All of the pages except the first one requires the User to be logged in. In this case I have actively logged the User in before the real page loading tests were made. 

Figure \vref{fig:shredhubPageTest} shows the results for loading www.shredhub.com.
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/shredhubPageTest.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for www.shredhub.com}
\label{fig:shredhubPageTest}
\end{figure}
Architecture 2.0 is very much slower then Architecture 1.0. The reason is that Architecture 2.0 has to load a big pile of JavaScript (the whole app!) before the server can start to render the real page. This is clear from the waterfall figure; once the App has loaded, the browser will start executing the JavaScript statements in the App, which will create templates and execute an Ajax request for Shreds in the database. On the other hand, Architecture 1.0 spends more time processing on the server, and the User has to wait ~532 milliseconds before he can see anything at all on the screen. An advantage with Architecture 2.0 here, is that the User can see a minor part of the page already after ~249 milliseconds. But this is just the scaffolding Html that is contained in Index.html, which is basically just a ``Shredhub'' headline. However, it does give the User something to look at much quicker then for Architecture 1.0. A final notice is that Architecture 2.0 sends more data to the browser. This is primarily because it sends the whole App.

Figure \vref{fig:shredpoolPageTest} shows the results for loading 
www.shredhub.com/shredpool.
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/shredpoolPageTest.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for www.shredhub.com/shredpool}\label{fig:shredpoolPageTest}
\end{figure}
Architecture 2.0 is displayed a little bit faster in the browser then Architecture 1.0, however the reason it is somewhat slow is because it has to perform 8 Ajax requests. The browser executes these in parallel (hence the 2180/8 ms for time spent on server), but it is still very time consuming. Architecture 1.0 is slow because it has to perform an Http redirect after authenticating the User. The real work happens during the work on rendering the Shredpool on the server. Altogether, Architecture 1.0 spends less time on the server, but is slower because of the redirect. Also, Architecture 1.0 sends more data. This is because the Shredpool html file fully rendered is quite big.

Figure \vref{fig:shreddersPageTest} shows the result for loading 
www.shredhub.com/shredders.
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/shreddersPageTest.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for www.shredhub.com/shredders}\label{fig:shreddersPageTest}
\end{figure}
Architecture 2.0 is almost twice as fast as Architecture 1.0. This is because Architecture 2.0 only fetches a small set of JSON Shredders from the server, and executes only a little bit of JavaScript in order to render the new page in the browser.  As for Architecture 1.0, even though the execution on the server is quite fast, the result shows up late in the browser  because the page that is sent is quite big and it is time consuming for the browser to render the whole page. Again, Architecture 2.0 sends much less data to the browser then Architecture 1.0. It's just an array of 20 JSON Shredders.

Figure \vref{fig:shredderPageTest} shows the results for loading 
www.shredhub.com/shredder/<uid>.
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/shredderPageTest.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for www.shredhub.com/shredder<uid>}\label{fig:shredderPageTest}
\end{figure}
Again, Architecture 2.0 scores better, and the reasons mostly the same as in the previous test. The App merely has to perform a tiny bit of JavaScript in order to create the new page, and there is just one Ajax call to the server in order to fetch the necessary data. Notice however that the amount of time it takes after the Ajax request has finished and the last JavaScript execution. This time difference is bigger then in the previous test, because the App has to execute some extra business operations in order to figure out the relationship between the visited Shredder, and the logged in User. Architecture 1.0 on the other hand is slow for the same reasons as in the previous example; the page that is created is big, and the browser has to render it from the ground up once it is received from the server. Also, like Architecture 2.0, Architecture 1.0 spends some time performing business operations before it starts rendering the Jsp page and sends it to the browser. It also sends more data to the browser, yet again because of the large html file. 


\subsection{Test 2 - Interactive User-Action Tests}
\textit{Goal: Determine the response time for interactive user-actions on Shredhub} \\
\textit{Tests: Response time/round-trip time}\\
This test is investigating the response time spent when a User performs a particular interactive task on Shredhub. Unlike test 1, which is investigating response times when complete Web pages are requested, this test only concerns minor interactive actions that happens inside a page, possibly without any server involvement. Again, the results are showed in waterfall models that capture the various phases for each request. The results also shows the time spent on the server, the time the end result was displayed in the browser, and the amount of data sent to the browser.

The user-actions tested are:
\begin{enumerate}
\item \textit{The User clicks next on a shred row}
\item \textit{The User clicks on a Shred that opens a new video window}
\item \textit{The User comments a shred}
\item \textit{The User rates a shred}
\end{enumerate}
Just like in Test 1, the Chrome development tool to investigate the round-trip time for each action. 

Figure \vref{fig:nextshredrow} shows the result for User-action 1. 
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/nextshredrow.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for when the User clicks on next shred row}\label{fig:nextshredrow}
\end{figure}
% 2.0 faster because no server required.
% 1.0 slow because form submit leads to 
Architecture 2.0 is very much faster in this case. The reason is that it doesn't have to consult the server; the next set of Shreds was fetched when the Shredpool was accessed, so it lives in the browser's JavaScript heap memory. It only has to execute some JavaScript code in order to alter the Dom to show the next row of Shreds. Architecture 1.0 is slower because it has to make an Http request to the server. Even though the next set of Shreds are cached in the session, the Http round-trip adds to the cost of displaying the page. Notice also that Architecture 1.0 does a page refresh here, because it is a form request, not Ajax. That explains why so much data is sent from the server. 

Figure \vref{fig:openshredvindow} shows the result for User-action 2. 
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/openshredvindow.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for when the User opens a Shred window}\label{fig:openshredvindow}
\end{figure}
The results are basically the same as in the previous test. The only difference is now Architecture 1.0 spends even more time on the server, because it has to fetch the Shred from the database.  

Figure \vref{fig:commentonshred} shows the result for User-action 3. 
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/commentonshred.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for when a User comments a Shred}\label{fig:commentonshred}
\end{figure}
In this case the results are fairly equal. However Architecture 1.0 is a tiny bit faster, simply because it doesn't execute as many JavaScript statements as Architecture 2.0. The reason for this is that the JavaScript in Architecture 1.0 are simple self-contained handler functions of less then 10 lines of code. The same functionality in Architecture 2.0 is implemented as part of a bigger code base, and has to go through several function calls and object instantiations in order to execute the action. Also, note that the times spent on the server are fairly equal for both prototypes. This makes sense considering that there is no business operations performed for Architecture 2.0, and Architecture 1.0 performs just a tiny amount of validation operations.


Figure \vref{fig:rateshred} shows the result for User-action 4. 
\begin{figure}
\begin{center}
\fbox{\includegraphics[width=13cm]
{images/rateshred.png}}
% Kommandoen \fbox tegner en ramme.
\end{center}
\caption{Test results for when a User rates a Shred}\label{fig:rateshred}
\end{figure}
The results here are fairly equal to the previous test. Architecture 1.0 scores a little bit better because there is a lot less JavaScript to execute in order to handle the request. 

\subsection{Test 3 - Back-end Scalability Test}
\textit{Goal: Determine how many concurrent requests the prototypes can support, and how quick the server handles requests under heavy load.} \\
\textit{Tests: Scalability of the back-end implementations}\\
This test was performed by creating multiple threads that executes a set of predefined actions on Shredhub. The actions are meant to simulate a normal flow of user-actions, to get a best-as-possible view of how well the server scales under common user-scenarios. The tests were created and executed with Apache JMeter. This was configured to have one test case that issues many subsequent actions: 

\begin{enumerate}
\item \textit{The User visits the home page}
\item \textit{The User logs in and visits the Shredpool}
\item \textit{The User uploads a shred}
\item \textit{The User watches a shred}
\item \textit{The User comments a shred}
\item \textit{The User accesses the page www.shredhub.com/shredders}
\item \textit{The User clicks on a particular shredder, which leads to the page www.shredhub.com/shredders/<uid>}
\end{enumerate}

Each thread (that is, a User) executes all these actions on the server, once. In order to perform stress testing,  JMeter is  set up to generate an increasing amount of simultaneous threads until the server starts to return erroneous responds. Now, JMeter needs a given amount of time in order to be able to create enough threads without saturating the test computer. This is called the ramp-up time. JMeter was configured to create the number of threads T with a ramp-up time = N seconds, where N = T for values of T from 1 to 100. All subsequent amount of threads T will be created with a ramp-up time of 100 seconds. The ramp-up period was configured this way in order not to create an unusual high hit rate on the server, which would be an undesirable behavior. The client test machines weren't able to issue more then 300 simultaneous threads executing the test case. Therefore, I had to add a new test machine for every n*300 threads. The results are showed in tables \vref{load1} and \vref{load2}.
\begin{description}
  \item[Users] = Number of concurrent threads
  \item[Throughput] = Average requests per second
  \item[Avg Request Time] = Average seconds spent per Http request
  \item[Error] = Average percentage of requests with error
\end{description}

\begin{table}
\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
	Users & Throughput & Avg Request Time & Error \\ \hline
	1 & 0.05 & 3.7 & 0 \\ \hline
	5 & 0.25 & 4.1 & 0 \\ \hline
	10 & 0.58 & 2.7 & 0 \\ \hline
       50 & 1.96 & 2.7 & 0 \\ \hline
	100 & 2.88 & 3.5 & 0 \\ \hline
	200 & 3.2 & 15.3 & 0 \\ \hline
       400 & 2.06 & 59.5 & 0 \\ \hline
	600 & 1.97 & 99.8 & 0.05 \\ \hline
    \end{tabular}
	\label{load1}
    \caption{Load testing results for Architecture 1.0}
\end{center}
\end{table}

\begin{table}
\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
	Users & Throughput & Avg Request Time & Error \\ \hline
	1 & 0.06 & 0.6 & 0 \\ \hline
	5 & 0.27 & 0.4 & 0 \\ \hline
	10 & 0.53 & 0.43 & 0 \\ \hline
       50 & 2.27 & 1.45 & 0 \\ \hline
	100 & 3.95 & 2.5 & 0 \\ \hline
	200 & 3.8 & 3.3 & 0 \\ \hline
       400 & 5.9 & 9.7 & 0 \\ \hline
	600 & 4.54 & 15.1 & 0 \\ \hline
	800 & 6.38 & 16.3 & 0 \\ \hline
	1000 & 5.21 & 17.1 & 0.07 \\ \hline
    \end{tabular}
    	\label{load2}
        \caption{Load testing results for Architecture 2.0}
\end{center}
\end{table}
The results show that Architecture 2.0 can handle more concurrent Users then Architecture 1.0. In addition, Architecture 2.0 has relatively quick request times compared to Architecture 1.0. A misfortune with Architecture 1.0 is that after 100 Users, the request times were very high. The reason for this is mainly because of the high memory consumption on the server when the number of active Users is high.   The server can handle them, but the processing time is slow. I chose to stop increasing users after respectively 600 and 1000, because after this, the error percentage increased drastically. A final thing to mention that is not depicted in the table, is that PostgreSQL applies caching such that for read queries, the fetching times are reduced by ~50\% after the first time a given query is executed. 

%
%\subsection{Test 5 - System profiling}
%\textit{Determine the throughput and efficiency in the various parts of the system to identity possible bottlenecks} 

\subsection{Test 4 - Database performance and scalability}
\textit{Goal: Determine how fast the most common database queries in Shredhub executes}\\
\textit{Tests: Database speed}\\
This test investigates the speed differences in using a document-oriented database versus a SQL database for Shredhub. The test is done by timing the query times spent when a User performs a given action that requires a database operation. A set of the most commonly used database queries are used as test cases. The timings start from the time the query instruction is made by the calling database handler, and ends when the result is mapped to a domain object (i.e a Java object for Architecture 1.0, and JSON object for Architecture 2.0). The test cases are designed to inspect at least one query from each of the four CRUD operations. Multiple Read operations have been checked, considering there are many types of read operations in Shredhub. The results are showed in table \vref{CrudResults}. 
\begin{table}
\begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
	Query & CRUD & Architecture 1.0  & Architecture 2.0 \\ \hline
	ShredsByRating & R & 244ms & 241ms \\ \hline
	ShredsByFanees & R & 11ms & 68ms \\ \hline
	ShredsByFaneesOfFanees & R & 202ms & 397ms \\ \hline
       ShredsByTags & R & 212ms & 106ms \\ \hline
	CreateShred & C & 64ms & 40ms \\ \hline
	AddShredRating & U & 27ms & 16ms \\ \hline
       DeleteShredComment & D & 33ms & 11ms \\ \hline
    \end{tabular}
    	\label{CrudResults}
        \caption{CRUD }
\end{center}
\end{table}
The results are fairly equal, however, there are some differences. Operations for Architecture 2.0 that requires MongoDB to perform custom join operations, are generally slower. This concerns ShredsByFanees, and ShredsByFaneesOfFanees. Operations that require SQL to join while MongoDb does not have to join because of its nested structure results in performance gains for Architecture 2.0. This concerns ShredsByTags and create, update, and delete operations. 


\subsection{Test 5 - Code Flexibility Test}
In this section we propose a test case that is meant to test the code flexibility and maintainability in the two prototypes. In this test, a new interactive user feature is to be implemented on Shredhub. The test is designed in a way that involves the modification of the User interface, implementing a new business process, and alteration of the database. The results for this test outlines the number of code statements that were added and modified, and the programming language(s) that were used to implement the feature. The new user feature that was implemented is given below.

\paragraph{User feature: The Guitar Showroom}
The Guitar showroom is part of the User's profile page, where the User can have pictures of his guitars,  and other Users can view the images one-by-one by scrolling sideways using left and right arrow buttons. The scrolling must be highly interactive, meaning no page refresh can happen.  Also, for every picture, the User can choose to ``dig'' the guitar. By clicking ``digg'', the guitar earns a ``digg'' point. Digg points is a way for the User to show that he likes the guitar. Each time a User diggs a guitar, the owner also earns one experience point. The User is not allowed to dig one of his own guitars. 

For testing purposes, a set of the existing test Shredders are to add an image for one of their guitars, and a fictive digg-value (i.e change some of their guitars to showroom guitars).

\subsubsection{Result for Architecture 1.0}
%
%In Shredder.jsp, 47 lines was added to display the guitars in Html + 1 line modified for maintainability of the list of guitars.
%In Shredder.jsp 81 lines of JavaScript was added to implement highly interactive behavior. No code-reusing techniques was successfully applied.
%In ShredderController: 10 lines to handle http request and call business logic function.
%In ShredderService: 16 lines to implement biz function and call db to persist.
%In ShredderDAOImpl: 9 lines to implement the SQL call + 4 line to maintain the other sql calls that uses guitar for Shredder.
%In PostgresDatabase: GuitarForShredder table had to be altered (=2 SQL lines added)
%In ShredderMapper: 4 Lines added to map the new GuitarForShredder table. + One line was deleted (maintainability).
%A new domain object had to be implemented: GuitarForShredder: 34 lines.

The code that was added and modified in order to implement the feature is given in the table below:
\begin{description}
  \item[Jsp] + 47 lines (Html + Jsp script statements)
  \item[JavaScript] + 81 lines. 
  \item[Java] + 73 lines, - 10 lines 
  \item[SQL] + 1 table, - 1 column + 1 column migration for every Shredder.guitars[] 
\end{description}

In the JavaScript implementation, no code reuse was successfully implemented. Therefore, 4 new specialized and tightly coupled JavaScript functions were added to the front-end codebase. The back-end however, did not require much modification, except for the database mapping code that was required in order to handle the change in the SQL structure.

In the Java codebase, a new controller handler, service function and DAO function had to be implemented. In addition, the ShredderMapper class had to be modified in order to handle the update in the SQL structure. Also, a new domain class had to be created to hold the new Sql update: GuitarForShredder.java. Now, every Shredder has an array of GuitarForShredder objects, as opposed to earlier, when every Shredder had a simple String array of guitar names. 

In SQL, a new table had to be generated to hold an image string, digs int, and name string, and a reference to the Shredder who owns the guitar. Previously every Shredder table had a simple array of strings that represented the guitars they own. Now, in order to implement the new update in Sql, I first had to create the new table GuitarForShredder, then for each Shredder, move every guitar column into a new row in the GuitarForShred table, then finally remove the guitars array in the Shredder table. 
The alteration of a Shredder is given in the example below: 
\begin{lstlisting}[language=SQL]
// Old Shredder table:
Column	| Type	|
guitars	| text[] 	|

// Showroom guitar
Column	| Type				|
guitar	| character varying(50) 	|
shredderid	| integer				|
imgpath    | character varying(20) 	|
digs      	| integer				|
\end{lstlisting}



%
%Important things:
%Had to add many new javascript functions. The size of the javascript code base is starting to grow %without structure. It is turning into spagetti code, because they don't have any structure and contains %both js, jsp and html. Some modification had to be implemented on the server side, this concerns %especially the database where the SQL table had to be modified, and the mapping required a lot of %code, including the creation of a new Domain class to wrap guitar for shredder object. Otherwise fairly %easy to add the new functionality on the backend.

%Sum: \textit{Added: 203 lines, Modified: 6 lines}

\subsubsection{Result for Architecture 2.0}
%In the Shredder.html: Added 40 lines of Html to display the guitars, + 4 lines modified displaying a different list of guitars (maintainability).
%In Shredder.model.js: 14 lines (increase shredder level was reused). Not include error message if same user tries to add.
%In ShredderView: 74 for mouse events and moving image. No need to do more then mouse event stuff and sliding image stuff. Listening to change events already happens, so the UI changes whenever a digg event is created.
%No need to modify anything. Models and Views are very open for extensions. Adding new UI changes was just to add more event handlers. An event handler calls a model function for business functions. the business function triggers an event when it finishes. The view listens to this event and rerenders automatically when the model causes a change that requires the view to render. This was already implemented ( view listens for update in shredder level events and renders thereafter). Should actually use mediator here! Use this instead of listening to change events directly.
%In ShredderController: 13 lines for handling api call and calling db.
%In Shredder.js : 6 lines for calling db. no need to change the database itself. Could added more control statements but the basic functionality is there. No modification of database or api needed.
%
%Sum: \textit{Added 148 lines, Modified: 4 lines}

The code that was added and modified in order to implement the feature is given in the table below:
\begin{description}
  \item[Html] + 40 lines - 4 lines
  \item[JavaScript App] + 88 lines. 
  \item[JavaScript API] + 19 lines
\end{description}

A lot of view logic code on the front end was successfully reused. A couple of new event-handlers were added to the Shredder view, and a business operation was added to the Shredder model.

On the back-end, there was no need to alter the database, because of mongoDB's flexibility: Previously, every Shredder had an array of guitars as strings that represents their list of guitar (only the names, just like Architecture 1.0). Now, for every guitar that is to include an image and a dig int, the array index that used to represent that particular guitar could simply be altered to be a nested JSON object inside the array, instead of a simple string. Now the only necessary modification in MongoDb was the alteration of the guitars Array for those test Shredders that was to change a guitar from being just a name to a JSON object with name, image and dig value. There was no need to alter any of the other Shredders, or even alter any of the CRUD operations that touches the Shredder object. The alteration of a Shredder can be seen in the code below:
\begin{lstlisting}
// Old guitar array:
 Shredder {
	guitars : [``Gibson Les paul'', ``Fender Stratocaster'']
}

// Showroom guitar
Shredder {
	guitars: [``Gibson Les Paul'',
	{ name : ``Fender Stratocaster'',
	image : ``fenderStrat.jpg'',
	diggs: 34
	}]
}
\end{lstlisting}

Also, there was no need to alter the Shredders that had not yet added switched to showroom guitars, because the program now accepts both guitars as simple strings, and JSON objects. The only modification needed at all was actually in the Html template that uses the list of guitars: an if-else block has to be added in order to check if a guitar is a string or a JSON object. Hence the -4 Html lines in the listing above.

Also, the back-end had to add a new REST api function to handle the database update for adding a dig. 

\section{Summary}
In this chapter we have looked at the tests that were made in order to analyze and compare the two prototypes. Five different tests were created which analyzes performance, scalability and code flexibility/maintainability. The results mostly proved advantages for Architecture 2.0, but Architecture 1.0 also had some winning features.  The results will be discussed in the following chapter. %% Usikker p\aa \aa si dette.


