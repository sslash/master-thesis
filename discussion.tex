% I ikke Web 2.0 kan rendring p\aa server v\aere ok, fordi sider er mer statiske og krever ikke s\aa mye arbeid for \aa bli generert

\chapter{Discussion}
\section{Introduction}
In this chapter we discuss the results that were found in the previous chapter. We discuss the problem statement in respect to the results, and propose possible hybrid solutions that combine the strengths of the two architectures. At the end we discuss the strengths and limitations of the study, and how the results should have an impact on future practice and research.

\section{Page Rendering}
Test 1 and 2 clearly showed that rendering on the client gave faster response times for the user. The reason is that the browser maintains all the HTML that is ever needed for the whole \textit{App}. Therefore it does not have to ask the server for HTML when the user goes to a new page or performs an action that requires new HTML to be rendered. It will simply just be fetched from the browser memory, and merged together with the necessary JSON data. The client still has to fetch the JSON data from the server from time to time, before it can do all the rendering, but this is not a matter of large data quantities. What is even better, is that the JSON data is fetched asynchronously. This way, when the user goes to a new page, the JavaScript code will immediately show the new page, except for just the content that requires JSON data. When the server request returns, the JavaScript \textit{App} will finish the page by adding the JSON content to it. Thus the user sees part of the page request almost immediately, in which the rest comes reasonably quick after. Good examples are figure \vref{fig:shreddersPageTest} and \vref{fig:shredderPageTest}, where the whole page is completed as soon as the browser receives JSON data from the server (even though part of the page is visible even earlier).

Also, when the client performs the rendering, it can choose to render only the parts that are necessary in order to display the result of a user action. In the case for server-side rendering, it has to render the whole page in any case. In addition, this leads to browser page refreshes, which is an unfortunate user experience. 

%% Fiks denna setningen
Rendering on the client also limits the amount of bandwidth consumed, which is proved in Test 1 in the previous chapter. Looking at the numbers regarding the amount of data sent between the client and server, shows that the JSON-based solution in Architecture 2.0 consumes much less bandwidth. The reason is that the fine-grained JSON solution consumes far less data then the HTML-based solution from Architecture 1.0.

An important decision is whether to eagerly fetch all the HTML and JavaScript at first, or to fetch it (lazily) when needed. I argued that eager fetching was preferable for Shredhub, because when all the HTML templates were merged together and compressed, the complete size was small enough to send in the initial page load without it being too much. However, this has not been tested on clients with poor browser capabilities and/or poor networking hardware, and it might be that lazy fetching might be a preferable solutions for some clients. Also, if Shredhub is to grow extensively in size with new pages and user features, it will probably be desirable to fetch the resources lazily regardless.
 
% kjappere pga HTML i browseren
% mindre b\aanbredde brukt pga mindre data kvantitet
A disadvantage with server-side rendering is that it is a time consuming process, something that is clear from figures \vref{fig:shreddersPageTest} and \vref{fig:shredderPageTest} where a lot of time is spent on the server. An outcome of this is that Architecture 2.0 can handle many more simultaneous users, because the average request times are much higher (table\vref{table:CrudResults}). Thus, the architecture is a lot more scalable.  

Now, figure \vref{fig:shredhubPageTest} shows that rendering on the server results in faster response time for the initial page request. The reason for this is that in Architecture 2.0, the browser has to wait for the whole \textit{App} to be completely loaded in the browser before it can start fetching additional JSON data and render the page. This is very time consuming and the tradeoff can in some cases be too high, depending on where one sets the upper limit for initial page response times. A proposal for a hybrid solution in this case is where the home page is rendered on server and includes a reference to the JavaScript \textit{App} with a ``async'' tag such that the \textit{App} is fetched lazily after the page is rendered in the browser. The result might be that the home page is rendered faster and the application will still have the thick-client solution for the rest of the user-interactions.  

\section{State and Business Logic on the Client}
Moving application state and business logic to the client has clear performance advantages for Shredhub. The browser stores state-data in its JavaScript memory and local storage, which in many cases avoids the need to consume the server for data. Good  examples that proves this are in figures \vref{fig:nextshredrow} and \vref{fig:openshredvindow}, in where the browser doesn't even have to fetch data from the server. When the client does have to fetch data, it consumes the API, which is always done asynchronously with AJAX, and in effect doesn't lead to any browser page refreshes. The outcome is a much faster Web app, with a highly interactive user experience where the user almost doesn't notice the HTTP requests. This naturally leads to better scalability on the server, as less time is spent there.

One disadvantage with this, however, is that it requires a lot of JavaScript code in order to keep the code base maintainable, which might lead to a lot of source code that is sent to the server. Much of the reason for this is that the JavaScript language lacks some important language features, and there are still browser incompatibilities that require a lot of extra JavaScript frameworks to be downloaded to the browser. Figures  \vref{fig:commentonshred} and \vref{fig:rateshred} showed the advantages of the simple JavaScript handlers in Architecture 1.0, which led to slightly better response times. However, the tradeoff is that these simple JavaScript handlers do not facilitate code reuse or any modularity, because they are implemented as simple handler functions that merely does one specific task. In the long term, this might add up to tangled and messy code. Disadvantages with this might be that the code can be difficult to read, variables can be overridden due to scope issues, refactoring the code can become tedious, and the code is difficult to test. A clear indication of this was found in test 5 in the previous chapter, where 81 lines of JavaScript code was separated in a few tightly coupled JavaScript functions, in order to build a new interactive user feature for Shredhub. Architecture 2.0, on the other hand has a more intuitive and  coherent code base, because both the view logic and business logic is implemented in the same language. This way it is possible to gather view logic and business logic that concerns the same domain under the one module, and thus achieve more coherence, while at the same time keeping HTML templates highly clean and almost stripped from template script tags and JavaScript. Architecture 1.0 uses three different languages for view and business logic (JSP, JavaScript and Java), and much of the view logic is tightly coupled with the HTML. Thus the programming benefits for the view logic code are clearly superior for Architecture 2.0. 

Another advantage with Architecture 2.0 is that the use of the back-end API decouples the client from the server. In Architecture 1.0, each HTML form and hyperlink has an associated controller handler on the server, that returns a particular view. The API however, is more general in that it doesn't return a view, because it is up to the client caller to decide how to use the results. Also, most API calls are flexible in that they allow the caller to for example  define result sizes and page numbers. Now, the big advantage with this is that it allows other client users to use the API as well. This could be a future mobile app, or a  3rd party application that wishes to use Shredhub data. However, due to the three-layered architecture in Architecture 1.0, offering a similar API would simply be to build a new presentation layer on top of the domain logic layer, that supports RESTful operations and communicates a more fine-grained data format, such as JSON or XML. 

Now in addition, avoiding sessions on the server had a very high performance impact. In Architecture 1.0, the server had to maintain a large set of Java objects in memory, for every current user. This resulted in slow response times when the number of simultaneous users was high, and the server was not able to handle more then 600 simultaneous active users (see table\vref{table:CrudResults}).

Now, it is difficult to state how much of this limitation was caused by the amount of memory consumed for maintaining state and sessions on the server, and how much was due to the complex rendering processes that happens for every request. I acknowledge the fact that this thesis lacks a deeper investigation of this in order to be able to draw more concrete conclusions regarding the scalability issue for Architecture 1.0. A solution would be to use a profiling tool to inspect how much time was spent on state handling, versus template rendering, and to use a monitoring tool to verify the amount of memory consumption used in maintaining session objects. 

Another hybrid solution in this case would be to use Redis for Architecture 1.0 to store sessions and state data. This would limit the memory usage for the Java virtual machine that currently maintains this data. The success with using Redis for authentication in Architecture 2.0, might be transferable to Architecture 1.0, because the use case is very similar. 

An important lesson learned from the performance test is that HTML form submits primarily used to implement minor interactive behavior (like in figure \vref{fig:nextshredrow} and \vref{fig:openshredvindow}), should not result in complete HTML pages, but a more fine-grained server response like the ones in Architecture 2.0. A lot of unnecessary page generation is done just for a small  DOM alteration. 

Architecture 1.0 can also benefit from a similar JavaScript code structure like the one in Architecture 2.0, in order to avoid having the JavaScript add up to a pile of tangled and non-reusable functions. 

A disadvantage with having the server completely stateless becomes clear in figure \vref{fig:shredpoolPageTest}, in which case a lot of time is spent on the server fetching JSON from the database. Now, this is done over 8 different HTTP requests, which results in a high transmission overhead and slow response time. Even though the complete rendering process is faster for Architecture 2.0 from the user's perspective, a lot of time could have been saved if the server was aware of the 8 different database fetches that are required to display the Shredpool. Another solution would be to offer a more coarse-grained API function that simply fetches all the JSON data that is needed in order to build the Shredpool page on the client, given a Uid for the Shredder. The tradeoff here is that offering such coarse-grained API functions could  create tighter couplings between the client and the API, similar to Architecture 1.0.

Although the study shows many advantages for this thick-client architecture, there are some major pitfalls:
\begin{itemize}
\item{} Some business rules has to be duplicated on the server in order to prevent malformed user input. This could come intentionally from users who knows how to issue HTTP requests without using Shredhub.com's Web interface. 
\item{} The Web app might not perform as well on other client machines and browsers than the ones that was used for testing. This might be a serious pitfall, because the result might be that slow computers and/or old browsers execute the JavaScript code so slow that Architecture 1.0 might be a preferable solution in terms of performance. This is most likely a case for older smart phones and desktop computers. I also acknowledge the fact that the testing phase of this thesis should have been done more extensively on various computers and smart phones in order to support these statements. Unfortunately, I did not have the time to do this.
\item{} The architecture is also not optimal for search crawlers, in where crawlers inspecting Shredhub would find merely empty HTML tags without content. Now, this is not a very critical problem, because most of Shredhub's content is only to be viewable once logged in. However, some parts of Shredhub should be fully searchable on the Web, and therefore a better solution for this problem remains to be implemented.
\item{} Some services depend on the number of times a page is reloaded. Examples are advertisement services like Google AdSense\cite{google-ad}, and monitoring services like Google Analytics\cite{google-an}. With architecture 2.0, these services wouldn't get correct page load data, because the page only loads once, on the initial request. 
\end{itemize}

\section{NoSQL and SQL Implementation}
The database test for Shredhub was somewhat limited, however the results show some valuable points that are worth discussing. First of all, the MongoDB implementation does not perform particularly fast when manual join operations have to be done. This concerned the read operation get Shreds by fanees, in which the JavaScript caller first has to fetch the Shredder, extract his array of fanees, then do a fetch operation for all Shreds where the owner is in the set of fanees. Also, an even worse case is when it has to fetch Shreds made by fanees of a Shredder's fanees. Here, the JavaScript caller has to further fetch all the Shredder's fanees' fanees and search for Shreds where one of them is the owner. Now, I should have added an index for the owner of a Shred in order to speed up this execution. In PostgreSQL, the Shred owner is a foreign key, in which the database has already added an index for it.

Even higher speed results could have been achieved if I had chosen to add indexes for quicker sorting; for example, I should have added an index for the time a Shred was created, considering this is used as sorting key for most Shred and Shredder queries. This applies to both the SQL and MongoDB implementation. 

On the other hand, MongoDB has clear speed advantages in cases where join operations are avoided. This happens because the MongoDB implementation wraps many of the separated SQL tables from Architecture 1.0 into one big collection, and therefore avoids having to join multiple tables together. The results show that create, update and delete operations are generally faster on Shredhub, because they are all just operations on a single MongoDB document, as opposed to the separated tables in SQL. Even the range query ``get Shred by tags'' are faster, because tags is a nested string array inside every Shred. In Architecture 1.0, tags is a separate table in which case joins has to be done.

Also, Architecture 2.0 has a big programming satisfactory advantage; everything is written in JavaScript. This facilitates better and cleaner cooperation across the whole codebase. This is especially beneficial for the database wrapper, because the data structure used is JSON based, which makes possible to completely avoid database mappers. In Architecture 1.0, database mapper code makes up the majority of the back-end codebase. Now, because JSON is the data structure used as transmission medium in the API as well, no marshalling is needed here either. This way, the architecture saves a lot of source code lines. The point is proven in the number of lines of code in Architecture 2.0 versus Architecture 1.0: \textbf{10505 vs 15867}. 

Another big advantage with the MongoDB implementation, is that the domain can be persisted exactly as it appears in the user interface. This is because of MongoDB's schema-less approach, which makes the application more flexible. This came clear in the last test of the previous chapter. Here, a lot of manipulation had to be done on the SQL database in order to implement a new feature. In MongoDB, manipulation was completely avoided, because the database allows for collections of similar types to have different content (hence, schema-less).

A final observation is that using Redis made it possible to have a session-free server, and still authenticate users for every API request (except requests for the login page), without getting performance bottlenecks. A great advantage with this is that it facilitates a shared-nothing architecture, which again facilitates replicating the back-end to many server machines. This could lead to large performance and scalability gains. This is somewhat limited in the session-oriented architecture, because sessions do not apply so well in distributed deployments: If a session is created on one server, and the user is directed to another server in a later request, the session is not found, and the user will be directed to the login page. Also, if a server goes down, every session on that machine is lost. Now, there are solutions for these problems, but they still provide more distribution obstacles then Architecture 2.0 does. 

\section{Strengths and Limitations of the Study}
% Styrker
\subsection{Srenghts}
The thesis reveals many important aspects of modern Web architecture design. I believe the results are not specific to Shredhub, but demonstrate principles that are valuable for the traditional Web 2.0 applications discussed in the initial chapters. The thesis especially proves one important point: Moving demanding tasks to the client with JavaScript is not only feasible, but leads to increased scalability and improved response-times. This trend is still very newfangled, and lacks proven results. I also believe that the thesis reveals pros and cons for both architectures, and is not biased by my own experience. 

\subsection{Limitations}
One limitation with this study is that I do try to solve many different problems in one single project. One could argue that each problem statement does not have sufficient material to give unambiguous conclusions. I do state that the results from the tests are evident enough to answer the problem statement, however, I do acknowledge that I could have chosen to narrow the scope of the thesis. Parts of the reason why I studied all these different technologies was that I found it very educational. However, I should have tried to team up with another master's thesis student who could have done either the back-end or front-end part of the study. 

Also, the code flexibility/simplicity test is a bit limited, and would possibly belong in a separate thesis by itself. However, even though it was difficult to measure, I do believe some valuable points were proven. 

A final notice is that I have chosen not to focus particularly on various caching techniques, as this would be  too much work for this thesis. Caching is absolutely necessary for high scalability and performance, which is why I mention it here.  

\section{Related Work}
The thick-client Web architecture is not a particularly new concept. However, implementing them as pure large-scale JavaScript architectures is definitely a new approach, and therefore it is not a lot of research that benchmarks the performance benefits for this. Related work mostly concerns the use of AJAX technologies for improving modern Web applications.

In a study, Mesbah and Van Deursen\cite{mesbah2007migrating}, proposed a reverse-engineering technique for migrating a traditional Web application to a single-page application (a concept described in chapter 3). The study does not concern comparing the traditional approach with the single-page approach, it is merely a study of how one can move from one to the other.

Mazzetti \textit{et al}\cite{4530218} have implemented, in another study, the MIRAJ framework, a system built to test and validate how REST and AJAX architectures can be implemented to cooperate in providing a public Web API. The conclusion was that the architecture proposed was a valid solution for a modern Web application back-end.

In a study from 2009, Ohara \textit{et al}\cite{4919645} studied the data-centricity in a typical Web 2.0 application that incorporates heavy use of user-participation. The implementation has client-side presentation logic and AJAX for client-server communication. They concluded that the server uses much time in the data source layer doing database manipulation, because of frequent AJAX requests. This could lead to reduced performance and poor scalability.

As for database technologies, some studies have been done that compares SQL with various NoSQL databases. 
Barahmand and Ghandeharizadeh\cite{mongsql3} proposed a benchmarking system, BG, for databases. In the study, a BG benchmarking test for comparing performance of a MongoDB database, against a SQL database was made. Similar to this thesis, the study is based on a social networking application that is built with these two different databases. None of the solutions were superior, however when Memcached was added to the SQL solution, it had superior performance.

Cattell\cite{sqlscale} did a comparison of many NoSQL and SQL systems designed to scale over multiple servers. The study merely concerns architectural comparisons, and does no benchmarking. The conclusion does not favor any of the databases, but argues for their respective pros and cons. The result of this study was important for this thesis, as the comparisons were used as a basis to decide the database solutions I have chosen.


\section{Implications on Practice}
I believe this study is important, because there are still many Web application developers who swear to many of the concepts of \textit{Reference-model 1.0}. Also, many developers aren't aware of the possibilities to move the application to the client. In addition, many are not aware of the possibilities of the JavaScript programming language itself, for example that it is fully possible to build modular and flexible large-scale codebases with the language. This often leads to the type of JavaScript code that was implemented for Architecture 1.0, something I argue should be avoided. The thesis advocates the capabilities of modern browsers, which enable developers to build thick-client JavaScript Web apps. These thoughts are fairly new, and lacks research.

Even though the thick-client architecture doesn't necessarily fit every modern Web application, they are important concepts to contemplate. For example is rendering HTML on the client very relieving on the server, and leads to increased scalability and throughput. If a complete client-side rendering architecture is not an option, maybe choosing to perform some client-side rendering and some server-side rendering is possible. A possible hybrid solution could be to let the server be responsible for rendering the major pages, while the client renders smaller dynamic HTML changes inside the page.  I leave this question as a task for further research: The study of finding solutions for combining server-side rendering with client-side rendering. This work would involve identifying when it is worthwhile rendering on the client and vice versa. This could also help improving the results picked up by search crawlers, something that is very important for applications that are not behind a login barrier. 

In this thesis we saw pros and cons for both using MongoDB and SQL. Maybe, there are ways to combine these in the same Web application, with the purpose of finding hybrid solutions. We saw for example that MongoDB could be used in combination with Redis in Architecture 2.0. This is an open question, and I leave others with an encouragement to further look for application areas where NoSQL solutions can be combined with SQL in order to fulfill each other. This work would involve finding parts of a domain where a given technology doesn't properly fit, in which case another technology might. This would require a very high decoupling of the data that is to be persisted, in order to separate it in different databases. 

Finally, we saw that Architecture 2.0 could be built by using JavaScript both on the back-end and front-end. A problem, however, was that some business rules had to be implemented both on the back-end and the front-end. I did not try to look for a solution to merge these together with the intentions of avoiding code duplication. I do believe this could be possible considering it is only one overall programming language used, so therefore I leave this as a further research topic. 


\section{Summary}
\paragraph{Performance and Scalability}
In this thesis I found that rendering HTML on the client takes much load off the server, and therefore makes it more scalable, because less processing has to be done for each request. This mostly resulted in better response times because the client could choose only to render the parts of the page that are necessary, and doesn't have to ask the server for the HTML. I also found that Web apps can perform better by having state and business logic in the client, because it endorses the storage of database objects in the browser's memory, and therefore reduces the amount of server calls needed. Once it must consult the server, it happens asynchronously in the background without the user noticing any delay, because there is no blocking process. This has great scalability benefits, also because the server doesn't have to maintain session data in memory for every current user. Also, the thick-client architecture decouples the client from the server, which might facilitate replicating the back-end. Which again is likely to improve the scalability of the system.

Finally I found that MongoDB might have performance benefits over traditional SQL, because it's data structure can reduce the need to join documents together. However, it is much slower in cases where it has to perform joins. Also, using Redis was a good solution for the stateless server model, because it authenticates each request very quickly. 

\paragraph{Programmer Benefits}
The programming benefits for the thick-client architecture is that both the user-interface logic and business logic is implemented in the same language, which creates a more coherent and intuitive code base. This also made it easier to have them cooperate together, as opposed to the thin-client solution where these responsibilities are separated in three different languages. Also, both of the NoSQL databases gives a very satisfactory programming environment, because only one common programming language is used, and therefore avoids the need to implement data-marshalling. In addition, the NoSQL approach is very flexible and allows the domain to be persisted exactly as it appears in the user-interface.

\paragraph{Future Alternatives}
We also saw some possible hybrid solutions where principles from Architecture 2.0 can be applied in Architecture 1.0 and vice versa. The most alternative is to combine client-side and server-side page generation, in where the server generates the coarse grained pages, while the client is responsible for generating smaller parts of a page. Also, possible future studies involves investigating how SQL and NoSQL solutions can be combined in the same application in order to fulfill each other when the technologies themselves become a bottleneck. Another interesting study is in cases for pure JavaScript Web applications; investigating how source code can be shared between the front-end and back-end, in order to avoid the code duplication that occurs in Architecture 2.0. 


