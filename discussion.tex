\chapter{Discussion}
In this thesis I found that rendering Html on the client takes a lot load off the server, and therefore makes it more scalable, because less processing has to be done for each request. This mostly resulted in better response times because the client could choose only to render the parts of the page that are necessary, and doesn't have to ask the server for the Html. I also found that Web-apps can perform better by having state and business logic in the client, because it endorses the storages of database objects in the browser's memory, and therefore reduces the amount of server calls needed. Once it must consult the server, it happens asynchronously in the background without the user noticing any delay. It also has scaling benefits because the server doesn't have to maintain session data in memory. The programming benefits for this is that both view logic and business logic is implemented in the same language, which creates a more coherent code base. Finally I found that the SQL implementation performs faster for read operations that doesn't use joins. In those cases, MongoDB was faster because much data was gathered in the same document, which avoids having to join multiple documents in queries. Also, using Redis was a good solution for the stateless server model, because it authenticates each request very quickly. Both these databases facilitate a very satisfactory programming environment on the back-end, because only one programming language is used. 

\section{Page Rendering}
Test 1 and 2 clearly shows that rendering on the client gave faster response times for the User. The reason is that the browser maintains all the Html that is needed for the whole App. Therefore it doesn't have to ask the server for Html when the User goes to a new page or performs an action that requires new Html to be rendered. It can just be fetched from the browser memory, and merged together with the necessary JSON data. The client still has to fetch the JSON data from the server from time to time, before it can do all the rendering, but this is much less data then a complete Html page.  Good examples are figure \vref{fig:shreddersPageTest} and \vref{fig:shredderPageTest}, where the page is rendered as soon as the browser receives JSON data from the server, and the App has modified the Dom to display the new result. Also, when the client performs the rendering, it can choose to render only the parts that are necessary in order to display the result of a user action. In the case for server side rendering, it has to render the whole page in any case. In addition, this leads to browser page refreshes, which is a unfortunate user experience. 

Rendering on the client also limits the amount of bandwidth consumed, which is proved in Test 1 in the previous chapter. Looking at the numbers regarding data received, shows that JSON data representing only the data needed to display a particular request (Architecture 2.0) is generally much less of a data quantity then the complete Html page rendered (Architecture 1.0). An important decision is wether to eagerly fetch all the Html at first, or lazy fetch Html when its needed. I argued that eager fetching was preferable for Shredhub, because when all the Html templates were merged together and minified, the complete size was small enough to send in the initial page load without harming this request too much. However, if Shredhub is to grow extensively in size with new pages and user features, it will probably be desirable to fetch Html lazily. 
% kjappere pga html i browseren
% mindre b\aanbredde brukt pga mindre data kvantitet
A disadvantage with server-side rendering is that it is a time consuming process, something that is clear from figures \vref{fig:shreddersPageTest} and \vref{fig:shredderPageTest} where a lot of time is spent on the server. An outcome of this is that Architecture 2.0 can handle many more simultaneous users, because the average request times are much higher (table \vref{load1} and \vref{load2}), and more scalable.  

Now, figure \vref{fig:shredhubPageTest} shows that rendering on the server results in quicker response time for the initial page request. The reason for this is that in Architecture 2.0, the browser has to wait for the whole app to be completely loaded in the browser before it can start fetch additional JSON data and render the page. This is very time consuming and the tradeoff can in some cases be too high, depending on where one sets the upper limit for initial page response times. 

\section{State and Business Logic on Client}
Moving application state and business logic to the client has clear performance advantages for Shredhub. The browser stores state data in its JavaScript memory and local storage, which in many cases avoids the need to consume the server for data. Good  examples that proves this are in figures \vref{fig:nextshredrow} and \vref{fig:nextshredrow}, in where the browser doesn't even have to fetch data from the server. When the client does have to fetch data, it consumes the API, which is always done asynchronously with Ajax, and in effect doesn't lead to any browser page refreshes. The outcome is a highly interactive user experience where the user doesn't notice the http requests.

One disadvantage with this, however, is that it requires a lot of JavaScript code in order to keep the code base maintainable, which might lead to a somewhat large amount of JavaScript statement to execute in order to perform a simple task. Figures \vref{fig:openshredvindow} and \vref{fig:commentonshred} showed the advantages of the simple JavaScript handlers in Architecture 1.0, which led to slightly better response times. However, the tradeoff is that these simple JavaScript handlers does not facilitate code reuse or any modularity, because they are implemented as simple handler functions that merely does one specific task. In the long term, this might add up to tangled and messy code. A clear indication of this was found in test 5 in the previous chapter, where 81 lines of JavaScript code was separated in 4 and tightly coupled event handler functions was implemented, in order to build a new interactive user feature for Shredhub. Architecture 2.0, on the other hand has a more intuitive and  coherent code base, because both the view logic and business logic is implemented in the same language. This way it is possible to gather view logic and business logic that concerns the same domain under the one module, and thus achieve more coherence, while at the same time keeping Html templates highly clean and almost stripped from template script tags and JavaScript. Architecture 1.0 uses three different languages for view and busines logic (Jsp, JavaScript and Java), and much of the view logic is tightly coupled with the Html. 

Another advantage with this architecture is that the use of the back-end API decouples the client from the server. In Architecture 1.0, each Html form and url anchor tag has an associated controller handler on the server. The API however, is more general in that it doesn't return a view, because it is up to the client caller to decide how to use the results. Also, most API calls are flexible in that they allow the caller to for example  define result sizes and page numbers. Now, the big advantage with this is that it allows other client users to use the API as well. This could be a future mobile app, or a  3rd party application that wishes to use Shredhub data.

Now in addition, avoiding sessions on the server had a very high performance impact. In Architecture 1.0, the server had to maintain a large set of Java objects in memory, for every current user. This resulted in slow response times when the number of simultaneous users are high, and the server was not able to handle more then 600 simultaneous active users (see figure \vref{load1}). Now, it is difficult to state how much of this limitation was caused by the amount of memory consumed for maintaining state and sessions on the server, and how much was due to the complex rendering processes that happens for every request. I acknowledge the fact that this thesis lacks a deeper inspection of this in order to be able to draw more concrete conclusions regarding the scalability issue for Architecture 1.0. A solution would be to use a profiling tool to inspect how much time was spent on state handling, versus template rendering, and to use a monitoring tool to verify the amount of memory consumption used in maintaining session objects.

A disadvantage with having the server completely stateless becomes clear in figure \vref{fig:shredpoolPageTest}, in which case a lot of time is spent on the server fetching JSON from the database. Now, this is done over 8 different http requests, which results in a much transmission overhead and slow response time. Even though the complete rendering process is faster for Architecture 2.0 from the User's perspective, a lot of time could have been saved if the server was aware of the 8 different database fetches that are required to display the shredpool. Another solution would be to offer a more coarse-grained API function that simply fetches all the JSON data that is needed in order to build the Shredpool page on the client, given a Uid for the Shredder. The tradeoff here is that offering such coarse-grained API functions could  create tighter couplings between the client and the API, similar to Architecture 1.0.

Although the study shows many advantages for this thick client architecture, there are some major pitfalls:
\begin{itemize}
\item{} Some business rules has to be duplicated on the server in order to prevent malformed user input. This could come intentionally from users who knows how to issue Http requests without using Shredhub.com's web interface. Now, this was only implemented for a few Http requests in Architecture 2.0; just enough to make me aware of the drawback.
\item{} The Web app might not perform as well on other client machines and browsers then the ones that was used for testing. This might be a serious pitfall, because the result might be that slow computers and/or old browsers executes the JavaScript code so slow that Architecture 1.0 might be a preferable solution in terms of performance. This is most likely a case for old smart phones and desktop computers. I do also acknowledge the fact that the testing phase of this thesis should have been done more extensively, on various computers and smart phones in order to support these accusations.
\item{} The architecture is also not optimal for search crawlers, in where crawlers inspecting Shredhub would find merely empty Html tags without content. Now, this is not a very critical problem, because most of Shredhub's content is only to be viewable once logged in. However, some parts of Shredhub should be fully searchable on the web, and hence a better solution for this problem remains to be solved.
\end{itemize}

\section{NoSql vs Sql}
The database test for Shredhub was somewhat limited, however the results show  some valuable points that are worth discussing. First of all, the MongoDB implementation does not perform particularly fast when manual join operations have to be done. This concerned the two read operations get Shreds by fanees, in which the JavaScript caller first has to fetch the Shredder, extract his array of fanees, then do a fetch operation for all Shreds where the owner is in the set of fanees. An even worse case is when it has to fetch Shreds made by fanees of a Shredder's fanees, I.e a second degree graph search. Here, the JavaScript caller has to further fetch all the Shredder's fanees' fanees and search for Shreds where one of them is the owner. Now, I acknowledge the fact that I should have added an index for the owner for a Shred in order to speed up this execution. Now, in PostgreSQL, the Shred owner is a foreign key, in which the dbms has already added an index for it. However, even higher speed results could have been achieved if I had chosen to add indexes for quicker sorting; for example, I should have added an index for the time a shred was created, considering this is used as sorting key for most Shred and Shredder queries. This applies to both the SQL and MongoDB implementation. Another option for the MongoDB implementation was to further de-normalize the documents and investigate the possibilities to even further duplicate code in order to avoid doing manual joins. I acknowledge that I should have spent more time in the beginning, on designing a more performance-oriented document model. 

Another advantage with the SQL implementation is that it successfully applies caching techniques for certain read queries. Therefore, subsequent Shred-read queries after the initial ones, were cut down with at least 50 \%. Now, it maybe that this is only a matter of configuring the MongoDB server, however, I have not spent further time investigating this. 

On the other hand, MongoDB has clear speed advantages in cases where join operations are avoided. This happens because the MongoDB implementation wraps many of the separated SQL tables from Architecture 1.0 into one big object, and therefore avoids having to join multiple tables together. The results shows that create, update and delete operations are generally faster on Shredhub, because they are all just operations on a single MongoDB document, as opposed to the separated tables in SQL. Even the range query ``get Shred by tags'' are faster, because tags is a nested string array inside every Shred. In Architecture 1.0, tags is a separate table in which case joins has to be done.

Also, Architecture 2.0 has a big programming satisfactory advantage. Everything is written in JavaScript. This facilitates better and cleaner cooperation across the whole codebase. This is especially beneficial for the database wrapper, because the data structure used is JSON based, which makes possible to completely avoid database mappers. In Architecture 1.0, database mapper code makes up the majority of the back-end codebase. Now, because JSON is the data structure used as transmission medium in the API as well, no marshaling is needed here either. Hence, the Architecture saves a lot of source code lines. The point is proven in the number of lines in Architecture 2.0 versus Architecture 1.0: 10505 vs 15867 lines.

A final observation is that using Redis made it possible to have a session free server, and still authenticate users for every API request (except requests for the login page), without getting performance bottlenecks. A great advantage with this is that it facilitates a shared-nothing architecture, which again facilitates distributing the back-end to many server machines. This could lead to large performance and scalability gains. This is somewhat limited in the session-oriented architecture, because sessions does not apply so well in distributed deployments: If a session is created on one server, and the user is directed to another server in a later request, the session is not found, and the user will be directed to the login page. Also, if a server goes down, every sessions on that machine is lost. Now, there are solutions for these problems, but they still provide more distribution obstacles then Architecture 2.0 does. 

 

\section{Strengths and Limitations of the Study}
% Styrker
\subsection{Srengths}
The thesis reveals many important aspects of modern Web architecture design. I believe the results are not specific to Shredhub, but demonstrates principles that are valuable for traditional Web-apps. The thesis especially proves one important point: Moving demanding concerns to the client using JavaScript is not only feasible, but leads to increased scalability and higher back-end throughput. This trend is still very newfangled, and lacks research. I also believe that the thesis reveals pros and cons for both architectures, and is not biased by my own experience. 

\subsection{Limitations}
One limitation with this study is that I do try to solve many different problems in one single project. One could argue that each problem statement does not have sufficient material to give significant conclusions. I mean that the results from the tests are evident enough to answer the problem statement, however, I do acknowledge that I could have chosen to narrow the scope of the thesis. Parts of the reason why I studied all these different technologies was that I found it very educational. I should have tried  to team up with another master's thesis student who could have done the back-end or front-end part of the study. 

Also, the code flexibility/maintainability test is a bit limited. This could even be a complete thesis in itself. The results do prove some important points however; the importance of structuring JavaScript code on the front-end in order to avoid ending up with tangled and non-reusable JavaScript functions. Something that is fully possible with the JavaScript language itself, or by using open source frameworks. 

\section{Implications for Practice}
I believe this study is important, because there are still many Web application developers who swear to the concepts of \textit{reference-model 1.0}. Also, many developers aren't aware of the possibilities of the JavaScript programming language itself, for example that it is fully possible to build modular and flexible codebases. The thesis advocates the capabilities of modern browsers, which enables developers to build thick client JavaScript Web apps. These thoughts are fairly new, and lacks research.

Even though the thick client architecture doesn't necessarily fit every modern Web application, they are important concepts to contemplate. For example is rendering Html on the client very relieving for the server, and leads to increased scalability and throughput. If a complete client-side rendering architecture is not an option, maybe choosing to perform some client-side rendering and some server-side rendering is possible. I leave this question as a task for further research: The study of finding solutions for combining server-side rendering with client-side rendering. This could also help improving the results picked up by search crawlers, something that is very important for applications that are not behind a login barrier.

In this thesis we saw pros and cons for both using MongoDB and SQL. Maybe, there are ways to combine these in the same Web application, with the purpose for finding hybrid solutions. We saw for example that MongoDB could be used in combination with Redis in Architecture 2.0. This is an open question, and I leave with an encouragement to further look for application areas where noSQL solutions can be combined with SQL. 

Finally, we saw that Architecture 2.0 could be built by using JavaScript both on the back-end and front-end. A problem, however, was that some business rules had to be implemented both on the back-end and the front-end. I did not try to look for a solution to merge these together with the intentions for avoiding code duplication. I do believe this could be possible considering there is one overall programming language, so therefore I leave this as a further research topic. 

